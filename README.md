# Mini-RAG: Retrieval Augmented Generation Project

This is a mini project based on **Retrieval-Augmented Generation (RAG)** where a language model is combined with a vector database to answer user queries using relevant external context.

The main goal of this project was to understand how **LLMs + vector search** work together in real applications, instead of using a plain chatbot.

---

## ğŸ“Œ About this project

I built this project as part of an **online assessment for an interview**.  
At the time of starting, I was **new to RAG systems and vector databases**, but I tried my best to understand the concepts and implement them properly within the given time.

This project represents my **learning approach, problem-solving mindset, and willingness to explore new technologies**, even with limited prior experience.


---

## ğŸ“Œ Why I built this project

While studying about Large Language Models, I realized that:
- LLMs can hallucinate
- They donâ€™t know private or custom data
- Fine-tuning is expensive and slow

So I built this project to learn how **RAG solves these problems** by retrieving relevant information first and then generating answers.

This project helped me understand **system-level thinking** in AI applications.

---

## ğŸ§  How the system works (High Level)

User Question
â†“
Convert question to embedding
â†“
Search similar vectors in database
â†“
Fetch top relevant context
â†“
(Optional) Re-rank context
â†“
Send context + question to LLM
â†“
Final Answer

---

## âœ¨ Features

- End-to-end RAG pipeline
- Vector similarity search using embeddings
- Clean separation between retrieval and generation
- API-based backend using FastAPI
- Secure handling of API keys using environment variables
- Easy to extend or modify for other datasets

---

## âš™ï¸ Tech Stack Used
### Backend
- Python
- FastAPI
- GROQ API
- Pinecone (Vector Database)

### Frontend
- Simple Python-based interface
- Communicates with backend using REST APIs

---

## ğŸ“ Project Structure

Mini-RAG/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ api.py                                 # API endpoints
â”‚ â”œâ”€â”€ rag.py                                 # Main RAG pipeline
â”‚ â”œâ”€â”€ retriever.py                           # Vector retrieval logic
â”‚ â”œâ”€â”€ reranker.py                            # Re-ranking logic
â”‚ â”œâ”€â”€ embeddings.py                          # Embedding generation
â”‚ â”œâ”€â”€ vectorstore.py                         # Vector DB handling
â”‚ â”œâ”€â”€ llm.py                                 # LLM interaction
â”‚ â”œâ”€â”€ config.py                              # Configurations
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ app.py                                 # Frontend interface
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE


---

## ğŸ› ï¸ How to run locally

### 1. Clone the repository
```bash
git clone https://github.com/Gurmukh1412/Mini-RAG.git
cd Mini-RAG

2. Setup backend
cd backend
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
pip install -r requirements.txt

3.Create a .env file:

OPENAI_API_KEY=your_api_key
PINECONE_API_KEY=your_api_key
PINECONE_ENV=gcp-starter
PINECONE_INDEX=text-rag-index


4.Run the server:

uvicorn api:app --reload

5.ğŸŒ API Access

After running the backend, API documentation is available at:

http://127.0.0.1:8000/docs

ğŸ¯ What I learned from this project
        How vector databases work
        How embeddings are used for semantic search
        How RAG reduces hallucinations in LLMs
        How to design modular AI systems
        How to structure an ML project properly

ğŸš€ Future Improvements
        Add document upload support
        Improve retrieval accuracy
        Add UI instead of CLI
        Support multiple datasets

ğŸ“„ License
This project is licensed under the MIT License.

ğŸ‘¤ Author
Gurmukh Singh
Undergraduate Student, IIT Mandi
